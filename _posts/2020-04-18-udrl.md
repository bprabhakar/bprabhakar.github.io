# Training Agents using Upside-Down Reinforcement Learning
<p align="center">
<a href="https://arxiv.org/abs/1912.02877">Paper</a>
â€¢  
<a href="https://github.com/bprabhakar/upside-down-reinforcement-learning">Github Repo</a>
</p>

Last year I attended the NeurIPS 2019 conference at Vancouver where I got to listen to tons of interesting ideas about machine learning. I've summarized my favorite ones in an earlier post. This post explores introduces one of those ideas (Upside-Down RL) in more detail, followed by a few results from my own attempt at re-implementing the algorithm.

## What is Upside-Down Reinforcement Learning (UDRL)?
Here is an excerpt from the pre-print's abstract:
>Traditional Reinforcement Learning (RL) algorithms either predict rewards with value functions or maximize them using policy search. We study an alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that solves RL problems primarily using supervised learning techniques. Here we present the first concrete implementation of UDRL and demonstrate its feasibility on certain episodic learning problems. Experimental results show that its performance can be surprisingly competitive with, and even exceed that of traditional baseline algorithms developed over decades of research.





## Sparse Lunar Lander
OpenAI Gym does not offer Sparse Lunar Lander as a default environment. I manually converted `LunarLander-v2` into a sparse environment by doing the following:
- For every non-terminal step, use a reward value of 0 when storing the transition in replay buffer
- For every terminal step, use the total episode reward value

![Sparse Lunar Lander](https://raw.githubusercontent.com/bprabhakar/upside-down-reinforcement-learning/master/plots/sparse_lunar_lander.png)

## Masking the commands in supervised learning step
![Masked Commands](https://raw.githubusercontent.com/bprabhakar/upside-down-reinforcement-learning/master/plots/sparse_lunar_lander_masked_cmd.jpeg)
